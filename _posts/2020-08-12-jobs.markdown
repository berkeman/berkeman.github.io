---
layout: post
title:  "Using the job object"
date:   2020-08-12 00:00:00
categories: processing
---

Job objects are used to pass dependencies, data and files between jobs
and build scripts.  Job objects are easy to use, and they bring a set
of functions that makes common tasks straightforward to write.



### Job objects are returned from build calls

The first place we encounter job objects is in build scripts.  The
Accelerator uses build scripts to execute jobs.  Jobs are stored on
the hard drive on the machine, so that they can be re-used if the same
build call is run again.  Here is an example of a simple build script:

```python
def main(urd):
    job = urd.build('csvimport', filename='mydata.csv')
```

This build script will create a job containing everything related to
the execution of the method `csvimport` with the option
`filename='mydata.csv`.  When execution finishes, the call returns a
**job object** representing the job.

### Job objects represent either existing jobs *or* the currently running job

Job objects come in **two very similar flavours**.  There is one used
to **reference existing jobs**, for example in build scripts, and one
used inside running jobs, used to **represent the running job
itself**.  The vast majority of functionality is available in both
places, but functions that only makes sense in one of the places is
unavailable in the other.


---

## 1. Job references

The most common use of the job object in a build script is to make the
job available to new jobs, like in this example

```python
    job_import = urd.build('csvimport', filename='mydata.csv')
    job_analyse = urd.build('analyse', source=job_import)
```

Here, `job_analyse` has access to everything related to `job_import`,
since it is using `job_import` as an input parameter.





---

## 2. Datasets

Datasets are created by and stored in jobs.



### Are there any Datasets in the job?

This will return a list of all datasets in the job

```python
    all_datasets = job.datasets
```



### How do I get a reference to a named dataset in a job?

To get a reference to a particular named dataset

```python
    ds = job.dataset(name)
```

An empty `name` corresponds to the default dataset, i.e. `name='default'`.



### How do I create a Dataset in a Job?

Using `job.datasetwriter()`, here's a sketch:

```python

def prepare(job):
    dw = job.datasetwriter(name='movies')
    dw.add('moviename', 'unicode')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res:
    ...
    dw.write(data)
```

Creating datasets is out of this posts' scope.  See the manual or
other examples or posts for more information.



---

## 3. Files


### How to load and store data in pickle or JSON formats

A running job can use `job.save()` to store data to a file in a job.
The data will be serialised using Python's pickle format.  For
example:

```python
def synthesis(job):
    data = ...

    job.save(data)  # using default name "result.pickle"
    job.save(data, 'anothername.pickle')

    return data  # store data as "result.pickle"
```

Note the last line - the data returned from synthesis is stored in the
file `result.pickle`!

Loading files from a job is similar, using `job.load()`, like this

```python
    data = job.load('anothername.pickle')

    data = job.load()  # to load "result.pickle"
```

There are also JSON-equivalents called `job.json_save()` and
`job.json_load()`.  The Accelerator typically uses JSON format
internally for meta data files and pickle format for everything else.



### Load a job's return value into a variable

As shown previously, the data returned by the `return` statement in
`synthesis()` is loaded like this

```python
    data = job.load()
```

Using `return` and `job.load()` is an easy way to pass data between
jobs and build scripts.



### How can I find the files stored in a job?

```python
    all_files = job.files()
```
This will return a `set` of all files created by the job.
WITH ABSOLUTE FILE NAMES



### How does the job know about what files that have been created?

The function `job.save()` as well as its JSON counterpart will
register its files automatically.  If files are generated by some
other method, files can be registered manually, like this:

```python
def synthesis(job):
    ...
    Image.save(filename)  # saves a file using PIL
    job.register_file(filename)
```

or, use `job.open` instead of plain `open`.

```python
def synthesis(job):
    ...
    with job.open(filename, 'wb') as fh:
        fh.write(mydata)
```


### How to get the absolute filename to a file in a job?

If the name of the file is known,

```python
   absname = job.filename(name)
```

will return the full pathname to the file.





---

## 4. File references

Passing jobs as input to other jobs is, as we've seen, straightforward.
But sometimes we only want to pass specific files from one job to
another.  Using a straight filenames would potentially hide the source
job and break transparency.  Instead, we pass information on the name
of the file *together* with information on the job creating it, using
the `job.withfile()` function, like this:

```python
    job = urd.build('create_a_file')
    urd.build('checkfile', thefile = job.withfile('somename')
```

In the corresponding method (in this case `checkfile`), it looks
something like this

```python
from accelerator import JobWithFile

options = dict(thefile=JobWithFile())

def synthesis():
    print('filename is', options.thefile.resolve())
```

(In addition, the `JobWithFile` object has functions to get filename
as well as jobname from the input parameter.)



---

## 5. Results

### Making a file in a job visible in the result directory

The result directory is a place where "global" can be placed.  We do
not store files there, because that would break the link to the job
creating the file.  Instead, we store links to files in job.  The
preferred way to do this is using the helper function
`job.link_result`, like this

```python
    job = urd.build('plotsomething', source=mydataset)
    job.link_results('plot.png')
```

This will create a soft link in the result directory pointing to the
file `plot.png` in `job`.

The name of the link can be set using the `linkname` option:

```python
    job = urd.build('plotsomething', source=mydataset)
    job.link_results('plot.png', linkname='distribution_of_taxicabs_on_mondays.png')
```



---



## 6. Miscellaneous, common for both existing and running job

```python
    job.method   # name of the job's method (i.e. corresponding source filename)

    job.path     # absolute path of job directory

    job.workdir  # name of workdir where the job is stored

    job.number   # the integer part of the jobid.
```


## 7. Miscellaneous, existing jobs

For existing jobs, there are these pieces of additional information
available through the job object as well:

```python
   job.output()  # A string containing everything the job has written to stdout and stderr!

   job.params    # A dict containing all the job's parameters

   job.post      # Execution times for all parts of the job
```


## 8.  Miscellaneous, current job

Information on location of input and result directory is available to
the running job object:

```python
    job.input_directory

    job.result_directory
```



---

## 9. Job Chains

Similar to dataset chains, it is possible to create chains of jobs.
This is done using the `jobs.previous` input parameter.  In a method:

```python
jobs = ('previous',)

def synthesis():
   ...
```

and in the build script

```python
    previous = None
    for ix in range(10):
        previous = urd.build('thejob', previous=previous)
```

This creates a job chain that is ten jobs long.  The
`job.chain()`-function will return a list of all jobs in the chain.

```python
    list_of_jobs_in_chain = job.chain()
```

Similar to dataset chains, this function takes some options, see
documentation for details.
