---
layout: post
title:  "The Job Object: The Key to Data and Results"
date:   2020-08-15 00:00:00
categories: processing
---


## Introduction

In Accelerator terms, a _job_ is an executed program, it contains
everything input as well as everything generated by a program run, and
it is stored on disk.  The job object is used to represent a job, and
it is keeper of links to datasets, files, parameters, and metadata.

The job object is used to create dependencies and pass data and files
between jobs and build scripts.  Job objects are easy to use, and they
bring a set of functions that makes common tasks straightforward to
write and comprehend.  This post gives an overview of most of the job
object's possibilities.



### Job Objects are Returned from Build Calls

The first place we encounter job objects is in build scripts.  The
Accelerator uses build scripts to execute jobs.  Jobs are stored on
disk, so that they can be used by other jobs and re-used if the same
build call is run again.  Here is an example of a simple build script:

```python
def main(urd):
    job = urd.build('csvimport', filename='mydata.csv')
```

This build script will create a job containing everything related to
the execution of the method `csvimport` with the option
`filename='mydata.csv`.  When execution finishes, the call returns a
**job object** representing the job.

### Job Objects Represent Either Existing Jobs *or* the Currently Running Job

To be specific, job objects come in **two very similar flavours**.
There is one used to **reference existing jobs**, for example in build
scripts, and one used inside running jobs, used to **represent the
running job itself**.  The vast majority of functionality is available
in both places, but functions that only makes sense in one of the
places is unavailable in the other.  The difference is kind of obvious
when working on a project, so it will not be mentioned again here.



---

## 1. Job References

The most common use of the job object in a build script is to make the
job available to new jobs, like in this example:

```python
    job_import = urd.build('csvimport', filename='mydata.csv')
    job_analyse = urd.build('analyse', source=job_import)
```

Here, `job_analyse` has access to everything related to `job_import`,
since it is using `job_import` as an input parameter.



---

## 2. Datasets

There is a tight connection between jobs and datasets.  Datasets are
created by, and stored in, jobs.


### Are There any Datasets in the Job?

This will return a list of all datasets in a job:

```python
    all_datasets = job.datasets
```



### How do I get a Reference to a Named Dataset?

To get a reference to a particular named dataset, use the `job.dataset()` function:

```python
    ds = job.dataset(name)
```

An empty `name` corresponds to the default dataset, i.e. `name='default'`.



### How do I Create a Dataset in a Job?

Datasets are created using `job.datasetwriter()`.  Creating datasets
is out of this posts' scope, but here's a sketch:

```python

def prepare(job):
    dw = job.datasetwriter(name='movies')
    dw.add('moviename', 'unicode')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res:
    ...
    dw.write(data)
```

See the manual or other examples or posts for more information.



---

## 3. Files

There are three ways for a job to store data: as datasets, files, or
using the return value, where the return value is actually a special case
of a file.


### How to Load and Store Data in Files

To simplify the coding experience, the Accelerator has built in
support for storing and retrieving data in Python's pickle format or
in JSON format.

A running job can use `job.save()` to store data to a file in a job.
The data will be serialised using Python's pickle format.  For
example:

```python
def synthesis(job):
    data = ...

    job.save(data)  # using default name "result.pickle"
    job.save(data, 'anothername.pickle')

    return data  # store data as "result.pickle"
```

Note the last line - the data returned from synthesis is stored in the
file `result.pickle`!

Loading data from a job is equally simple, using `job.load()`, like this:

```python
    data = job.load('anothername.pickle')

    data = job.load()  # to load "result.pickle"
```

A common example would be to run a job and then do something with its
output.  Assume that the `analyse` job returns some data using the
`return` statement, then we could do:

```python
    job_analyse = urd.build('analyse', source=job_import)
    data = job_analyse.load()
    # do something with data here...
```

*The power of using the pickle format is that it is possible to store
and retrieve complex datatypes between jobs and build scripts without
having to consider formatting or parsing!*

The JSON-equivalents are called `job.json_save()` and
`job.json_load()`.  (The Accelerator typically uses JSON format
internally for meta data files and pickle format for everything else.)

It should be mentioned that `job.open()` as well as the load and save
functions can work on **parallel files**, i.e. files that are written
in parallel independently in each slice by one job, and read back in
parallel in another job.  See manual for more information.




### How can I Find the Names of all Files Stored in a Job?

```python
    all_files = job.files()
```

This will return a `set` of all files created by the job.  This
function takes an optional `pattern` argument that is used to filter
the filenames:

```python
    only_the_text_files = job.files('*.txt')
```



### So, How does the Job Know what Files it Contains?

The function `job.save()`, as well as its JSON counterpart, will
register its files automatically to the running job.  If files are
generated by some other means, files can be registered manually, like
this:

```python
def synthesis(job):
    ...
    Image.save(filename)  # saves a file using PIL
    job.register_file(filename)
```

or, using the extended `job.open()` instead of plain `open()`:

```python
def synthesis(job):
    ...
    with job.open(filename, 'wb') as fh:
        fh.write(mydata)
```


### How to get the absolute filename to a file in a job?

If the name of the file is known,

```python
   absname = job.filename(name)
```

will return the full pathname to the file.



---
## 4. Transparency

There are several ways to inpect a job.

 - Using the Accelerator Board web server and inspect jobs and
   datasets from your web browser.  Launch it by typing  
   `ax board`

 - Use the `ax job <job>` command to get information on a specific job

 - All job meta-data that makes sense to a human is stored in
JSON-format, so it is possible to navigate to a job directory and
inspect the files directly, in particular, the files `setup.json` and
`post.json`.

Here is an example using the board server

<p align="center"><img src="{{ site.url }}/assets/board.png"> </p>


---

## 5. Results

The result directory is a place where "global" results and findings
can be put.  We use soft links, that *point* to files in jobs, instead
of putting actual files there.  The reason is that if a files is
copied from a job to the result directory, the connection between the
file and its origin (the job) is gone.  The difference between a file
and a link is that the link shows the actual location of the original
file (which is in the job that created it).  So the link maintains
transparency.


### Making a File in a Job Visible in the Result Directory


The preferred and easiest way to create links in the result directory
is to use `job.link_result()`, like this:

```python
    job = urd.build('plotsomething', source=mydataset)
    job.link_results('plot.png')
```

This will create a soft link in the result directory pointing to the
file `plot.png` in `job`.

The name of the link can be set using the `linkname` option:

```python
    job = urd.build('plotsomething', source=mydataset)
    job.link_results('plot.png', linkname='distribution_of_taxicabs_on_mondays.png')
```



---

## 6. File references

Passing jobs as input to other jobs is, as we've seen,
straightforward.  But sometimes we only want to pass specific files
from one job to another.  There are several ways to do this:

 - We pass the job object representing an existing job with files to a
   new job.  In this case, the new job will know which job that
   created and holds the files, but it will not know the name of the
   actual files.

   If there is only one file, and we use the convention to name the
   file `result.pickle`, there is no problem.  But if there are more
   than one file, or the file name is not always unique, the new job
   does not know how to resolve the situation.  One solution is to
   hard code the file names in the new job's source code.  *This is ok
   in many cases, but it is not a generic solution.*

 - We pass the full pathname of the file to the new job as a
   parameter.  In this case, the new job can read the file directly,
   but the information about which job that generated the file is
   gone.  *This breaks transparency and should be avoided.*

A third alternative is to pass the file name *together* with a
reference to the job that created it.  This is done using the
`job.withfile()` function, like this:

```python
    job = urd.build('create_a_file')
    urd.build('checkfile', thefile = job.withfile('somename')
```

Here, the `checkfile` job is working on the file named `somename`,
that was generated by the `create_a_file` job.

In the corresponding method (in this case `checkfile`), it looks
something like this

```python
from accelerator import JobWithFile

options = dict(thefile=JobWithFile())

def synthesis():
    print('filename is', options.thefile.resolve())
```

In addition, the `JobWithFile` object has functions to get filename as
well as jobname from the input parameter, and it also handles parallel
file sets for passing data between parallel processing jobs, just like
`job.open()` and the load and save functions.  See manual for more
information.



---

## 7. Miscellaneous Functions

### Miscellaneous Functions, Available for Existing Jobs

These functions are available on existing jobs to investigate a job's
output, input parameters, and execution times (covering both parallel
and serial parts of the program in detail).

```python
   job.output()  # A string containing everything the job has written to stdout and stderr!

   job.params    # A dict containing all the job's parameters

   job.post      # Execution times for all parts of the job
```



###  Miscellaneous Functions, Available for Running Jobs

Information on location of input and result directory is available to
the running job object:

```python
    job.input_directory
```

The reference to the `input_directory` is useful when writing data
import methods.  Storing data in the `input_directory`, specified by
the Accelerator's configuration file, makes it possible to import data
without absolute file path dependencies.  This makes it much easier to
port a project to a different system.



### Miscellaneous Functions, Common for both Existing and Running Jobs

These are only mentioned for completeness:

```python
    job.method   # name of the job's method (i.e. corresponding source filename)

    job.path     # absolute path of job directory

    job.workdir  # name of workdir where the job is stored

    job.number   # the integer part of the jobid.
```



---

## 8. Job Chains

Similar to dataset chains, it is possible to create chains of jobs.  A
job chain is created using the `jobs.previous` input parameter.  In a
method:

```python
jobs = ('previous',)

def synthesis():
   ...
```

and in the build script

```python
    previous = None
    for ix in range(10):
        previous = urd.build('thejob', previous=previous)
```

This creates a job chain that is ten jobs long.  The
`job.chain()`-function will return a list of all jobs in the chain.

```python
    list_of_jobs_in_chain = job.chain()
```

Similar to dataset chains, this function takes some options, see
documentation for details.



---

## 9. Relation to Other Classes

The graph below shows as simplified version of how the different
classes used by the Accelerator are connected.  The classes at the top
are used in build scripts, and classes at the bottom in running jobs.

<p align="center"><img src="{{ site.url }}/assets/classes.svg"> </p>

Note that

- only a small subset of all available class member functions are
shown in order to keep the image simple, and

- the graph does only show the most relevant classes.



---

## 10. Conclusion

The intention of this post is to show the various functions and most
common use cases for the job object.  The job object provides
streamlined solutions to common situations that reduce the amount of
code to write (and maintain).  For more details and practical
examples, see the references below.
